{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkBSHlPb0wFbIon+knVrwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oilportrait/test_colab/blob/main/CLIPTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSeIejaClYQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Joshua K. Cage 저자 임선집 옮긴이의 101가지 문제로 배우는 딥러닝 허깅페이스 트랜스포머 with 파이토치(Python Transformers By Huggingface Hands On:)의 예제를 실행한 것임을 알립니다.\n",
        "This code has been brought from the GitHub repository below and has been slightly modified.:https://github.com/jasonyim2/book3/blob/main/Transformers%2027-36.ipynb \"\"\"\n",
        "!pip install transformers\n",
        "!pip install ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 새주석: 로컬에 존재하는 사진을 코랩 환경으로 가져오기\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = next(iter(uploaded))\n",
        "image = Image.open(filename)"
      ],
      "metadata": {
        "id": "JG65mLzII3Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "0VDQXEg9JDcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 및 프로세서 불러오기\n",
        "## 새주석: .from_pretrained를 이용해서 원하는 이름의 모델을 트랜스포머 라이브러리에서 가져온다\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
      ],
      "metadata": {
        "id": "ArNdepW1JO5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 새주석: 가지고 온 모델의 상세 스펙을 출력합니다\n",
        "model"
      ],
      "metadata": {
        "id": "59F1UwYzJ1kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트를 리스트 타입으로 입력\n",
        "## 새주석: 이 텍스트는 사진이 분류될 케이스를 나누는것입니다.\n",
        "candidates  = [\"giraffe and zebra\", \"a zebra eating grass\", \"giraffe next to zebra\", \"zebra rides a giraffe\",\"zebra on top of a giraffe\"]\n",
        "\n",
        "# 프로세서(ClIPPprocessor)에 텍스트 및 이미지를 입력하여 인코딩\n",
        "## 새주석: 프로세서는 모델에 데이터를 넘겨주기 전에 전처리를 해줍니다\n",
        "inputs = processor(text=candidates, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# inputs 출력\n",
        "inputs"
      ],
      "metadata": {
        "id": "xliC0UGjJ7Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코딩한 이미지 출력\n",
        "## 새주석: .imshow는 프로세스를 거쳐서 나온 이미지 텐서를 그려주는 역할을 합니다.\n",
        "## 새주석: ['pixel_values']는 불러올 키값을 뜻하고 [0][0]에서 첫번째 0은 순서상 처음 이미지를 불러온다는 의미이며 두번쨰 0은 rgb의 r을 의미합니다.\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(inputs['pixel_values'][0][0]);"
      ],
      "metadata": {
        "id": "u-LjAW5KLCFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs['pixel_values'] 차원 확인\n",
        "## 새주석:(1, 3, 224, 224)을 각각 설명하면 1은 배치 사이즈이며 3은 색깔채널인 rgb를 표현합니다. 224는 이미지의 사이즈를 나타냅니다.\n",
        "inputs['pixel_values'].shape"
      ],
      "metadata": {
        "id": "7BxTXzhxLGMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코딩한 이미지 출력. 인덱스를 [0][1]로 변경\n",
        "## 새주석: 이번에는 기존과는 다르게 펏번쨰 이미지 rgb의 g부분을 출력합니다.\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(inputs['pixel_values'][0][1]);"
      ],
      "metadata": {
        "id": "2-t4sze7LJQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코딩된 텍스트 출력\n",
        "## 새주석:input_ids는 입력에서 텍스트를 어떻게 토큰화 시켰는지 알려줍니다.\n",
        "inputs['input_ids'][0]"
      ],
      "metadata": {
        "id": "lA4hC1CBLP7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 결과를 디코딩한 텍스트 출력\n",
        "## 새주석: 위에서 토큰화 시킨 텐서를 그대로 디코딩함으로서 어떤 텍스트를 넣었는지 확인합니다.\n",
        "processor.tokenizer.decode(inputs['input_ids'][0])"
      ],
      "metadata": {
        "id": "IBJgk3lULStH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 eval 모드로 전환\n",
        "model.eval()\n",
        "\n",
        "# **inputs에서의 ** 표시는 inputs 변수가 키(key)와 값(value)로 이루어져 있을 때\n",
        "# input 변수에 담긴 키와 값을 모두 모델에 입력하는 용도임\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# 출력물 outputs의 키(key) 출력\n",
        "## 새주석:logits_per_image는 모든 텍스트 프롬프트와 이미지가 얼마자 잘 일치되는지에 대한 점수를 측정한것입니다\n",
        "outputs.keys()"
      ],
      "metadata": {
        "id": "HNWvQZVxLW_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 새주석: 곧바로 분류를 위해서 필요한 키값인 logits_per_image를 가져옵니다.\n",
        "logits_per_image = outputs.logits_per_image\n",
        "print(logits_per_image)"
      ],
      "metadata": {
        "id": "IYm8iTJRLZYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logits_per_image에 담긴 값을 입력값 행별로(dim=1) 소프트맥스 함수에 투입\n",
        "## 새주석: 각 텍스트 프롬프트와 이미지의 유사성을 확률로서 산출하기 위한 소프트 맥스입니다.\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "import torch\n",
        "\n",
        "# 변수 probs에 담긴 값 중에 최고값의 인덱스를 argmax로 찾고\n",
        "# item()을 통해 레이블 즉 제목을 출력\n",
        "# 그 결과가 cadidates의 인덱스 값이 됨\n",
        "print(candidates[torch.argmax(probs).item()])"
      ],
      "metadata": {
        "id": "comXKmKGLcRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}